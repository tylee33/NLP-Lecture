{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylee33/NLP-Lecture/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvP3N4ChXLER"
      },
      "source": [
        "# Seq2Seq\n",
        "제작: [봉수골 개발자 이선비](https://www.youtube.com/channel/UCOAyyrvi7tnCAz7RhH98QCQ) \n",
        "\n",
        "1. Chacter level\n",
        "2. Word level\n",
        "3. Word levvel + Attention\n",
        "\n",
        "![이런거](https://raw.githubusercontent.com/KerasKorea/KEKOxTutorial/master/media/28_1.png)\n",
        "\n",
        "참고: https://wikidocs.net/24996"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiUbV8APaMsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6fe80f9-dde0-42dc-f055-b54313ade92e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-fra.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-06 03:29:41--  https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-fra.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9541158 (9.1M) [text/plain]\n",
            "Saving to: ‘eng-fra.txt’\n",
            "\n",
            "eng-fra.txt         100%[===================>]   9.10M  44.1MB/s    in 0.2s    \n",
            "\n",
            "2021-02-06 03:29:41 (44.1 MB/s) - ‘eng-fra.txt’ saved [9541158/9541158]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGNUNGD5ok6J"
      },
      "source": [
        "# 전처리\n",
        "- special character 제외 \n",
        "- 영문, 불어 문장 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JhKt_UDrCv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4032dc-83ee-4ae6-ed4c-6fbca10923e6"
      },
      "source": [
        "# 다음을 단어로 분리해보자.\n",
        "en_sent = u\"Have you had dinner?\"\n",
        "fr_sent = u\"Avez-vous déjà diné?\"\n",
        "\n",
        "en_sent = en_sent.replace('-', ' ').replace('?', ' ')\n",
        "fr_sent = fr_sent.replace('-', ' ').replace('?', ' ')\n",
        "\n",
        "print(en_sent.lower().split())\n",
        "print(fr_sent.lower().split())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['have', 'you', 'had', 'dinner']\n",
            "['avez', 'vous', 'déjà', 'diné']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncrZnsoTcDdb"
      },
      "source": [
        "## 소문자로 변환 후, 특수문자를 찾아서 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXM2seKmowqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a246c46-2cae-46b2-e8b4-f3ae647cff09"
      },
      "source": [
        "# 모든 스페셜 캐릭터를 찾아보자.\n",
        "with open('eng-fra.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "spcials = set(text.lower()) - set('abcdefghijklmnopqrstuvwxyzàâçèéêëîïòôöùúûœас\\t\\n\\' ')\n",
        "print(sorted(spcials))\n",
        "print(''.join(sorted(spcials)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', '\"', '$', '%', '&', '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '~', '\\xa0', '«', '\\xad', '»', '\\u2009', '\\u200b', '‐', '–', '‘', '’', '…', '\\u202f', '‽', '₂', '€', '\\u3000']\n",
            "!\"$%&()+,-./0123456789:;?~ «­» ​‐–‘’… ‽₂€　\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AECkc-Uurr51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd1ae7c-45b3-4441-dc0e-216ef7333a28"
      },
      "source": [
        "print(text[:100])\n",
        "\n",
        "# 모두 소문자로\n",
        "text = text.lower()\n",
        "\n",
        "# 스페셜 캐릭터 제거 \n",
        "text_alpha = text\n",
        "for s in spcials:\n",
        "    text_alpha = text_alpha.replace(s, ' ')\n",
        "\n",
        "print(text_alpha[:100])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Sto\n",
            "go \tva  \n",
            "run \tcours  \n",
            "run \tcourez  \n",
            "wow \tça alors  \n",
            "fire \tau feu  \n",
            "help \tà l'aide  \n",
            "jump \tsaute \n",
            "sto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFG7ZUS_b59w"
      },
      "source": [
        "## 영어 불어 문장 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bumQveJzXSHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c60cef1-fe9d-49c6-e925-119a77e9d343"
      },
      "source": [
        "## 줄로 구분\n",
        "lines = [line.split('\\t') for line in text_alpha.split('\\n')]\n",
        "print(lines[:10])\n",
        "print(len(lines))\n",
        "\n",
        "# 50000개 문장만 추출하여 shuffle\n",
        "import random\n",
        "lines = lines[:50000]\n",
        "random.shuffle(lines)\n",
        "\n",
        "# 영어 문장과 불어 문장을 분리 \n",
        "eng_sent, fra_sent = zip(*lines)\n",
        "eng_sent = [s.strip() for s in eng_sent]\n",
        "fra_sent = [s.strip() for s in fra_sent]\n",
        "print(eng_sent[:3])\n",
        "print(fra_sent[:3])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['go ', 'va  '], ['run ', 'cours  '], ['run ', 'courez  '], ['wow ', 'ça alors  '], ['fire ', 'au feu  '], ['help ', \"à l'aide  \"], ['jump ', 'saute '], ['stop ', 'ça suffit  '], ['stop ', 'stop  '], ['stop ', 'arrête toi  ']]\n",
            "135843\n",
            "['it was a mess', \"that's a good guess\", 'my house is your house']\n",
            "[\"c'était le bordel\", 'bien deviné', 'ma demeure est la tienne']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geF6gFVWsqLr"
      },
      "source": [
        "# seq2seq2 I - Charactor Level\n",
        "\n",
        "1. 데이터 준비\n",
        "  - tokenizing => 각 문자열에 매핑되는 숫자(인덱스) 구성\n",
        "  - padding => 길이를 맞춰준다. \n",
        "  - indexing => token을 기준으로 실제 문장을 숫자(인덱스)로 변환\n",
        "  - one hot encoding\n",
        "1. 모델 준비\n",
        "  - 모델 생성\n",
        "  - 모델 학습\n",
        "1. 모델의 이용\n",
        "  - 번역\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLaR2cT0XFbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3483e9e-cb31-46c3-8b7d-ceaf63938dd2"
      },
      "source": [
        "# start 문자열로 탭(\\t) 사용\n",
        "# end 문자열로 줄바꿈(\\n) 사용\n",
        "fra_sent = [f'\\t{s}\\n'for s in fra_sent]\n",
        "print(eng_sent[:3])\n",
        "print(fra_sent[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['it was a mess', \"that's a good guess\", 'my house is your house']\n",
            "[\"\\tc'était le bordel\\n\", '\\tbien deviné\\n', '\\tma demeure est la tienne\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQdSTi_zahys"
      },
      "source": [
        "## 데이터 준비: tokenizing - 알파벳 하나 하나 단위로 token 삼겠다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW8PPR2pZINk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef941844-0555-4a05-9bc7-f8e7e7f89847"
      },
      "source": [
        "eng_char = sorted(set(''.join(eng_sent)))\n",
        "fra_char = sorted(set(''.join(fra_sent)))\n",
        "\n",
        "print(f'영어 - 가장 긴 문장: {max(map(len, eng_sent))}, 사용 알파벳 수: {len(eng_char)}')\n",
        "print(f'불어 - 가장 긴 문장: {max(map(len, fra_sent))}, 사용 알파벳 수: {len(fra_char)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 - 가장 긴 문장: 24, 사용 알파벳 수: 29\n",
            "불어 - 가장 긴 문장: 73, 사용 알파벳 수: 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qJDpeeZOev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f26701-b0a0-488c-b068-024e0851d3c1"
      },
      "source": [
        "eng_token = {c: i for i, c in enumerate(eng_char)}\n",
        "fra_token = {c: i for i, c in enumerate(fra_char)}\n",
        "\n",
        "print(eng_token)\n",
        "print(fra_token)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{' ': 0, \"'\": 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27, 'é': 28}\n",
            "{'\\t': 0, '\\n': 1, ' ': 2, \"'\": 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, 'à': 30, 'â': 31, 'ç': 32, 'è': 33, 'é': 34, 'ê': 35, 'ë': 36, 'î': 37, 'ï': 38, 'ô': 39, 'ù': 40, 'û': 41, 'œ': 42, 'с': 43}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_juBWFP1bFVa"
      },
      "source": [
        "## 데이터 준비: padding => 길이를 맞춰준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us822sVBbE2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2799b9-fbfe-4c62-8cd3-1889b9270b11"
      },
      "source": [
        "eng_pad = [list(eng.ljust(24)) for eng in eng_sent]\n",
        "print(eng_pad[:2])\n",
        "\n",
        "fra_pad = [list(fra.ljust(73)) for fra in fra_sent]\n",
        "print(fra_pad[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', 'm', 'e', 's', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['t', 'h', 'a', 't', \"'\", 's', ' ', 'a', ' ', 'g', 'o', 'o', 'd', ' ', 'g', 'u', 'e', 's', 's', ' ', ' ', ' ', ' ', ' ']]\n",
            "[['\\t', 'c', \"'\", 'é', 't', 'a', 'i', 't', ' ', 'l', 'e', ' ', 'b', 'o', 'r', 'd', 'e', 'l', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '], ['\\t', 'b', 'i', 'e', 'n', ' ', 'd', 'e', 'v', 'i', 'n', 'é', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbRXP9FHbBs-"
      },
      "source": [
        "## 데이터 준비: indexing => token을 기준으로 실제 문장을 숫자로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlrfHZ2jcK1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568f51e5-a328-4473-ee0b-793ae41a9750"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "eng_idx = np.array([list(map(eng_token.get, eng)) for eng in eng_pad])\n",
        "print(eng_idx[:2])\n",
        "\n",
        "fra_idx = np.array([list(map(fra_token.get, fra)) for fra in fra_pad])\n",
        "print(fra_idx[:2])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10 21  0 24  2 20  0  2  0 14  6 20 20  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [21  9  2 21  1 20  0  2  0  8 16 16  5  0  8 22  6 20 20  0  0  0  0  0]]\n",
            "[[ 0  6  3 34 23  4 12 23  2 15  8  2  5 18 21  7  8 15  1  2  2  2  2  2\n",
            "   2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
            "   2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
            "   2]\n",
            " [ 0  5 12  8 17  2  7  8 25 12 17 34  1  2  2  2  2  2  2  2  2  2  2  2\n",
            "   2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
            "   2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
            "   2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8KjnuI1Lzb"
      },
      "source": [
        "## 데이터 준비: one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZxVnkq9uBHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee52f2f-c7cd-4e75-fe02-1e7f4bfc9484"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "eng_onehot = tf.keras.utils.to_categorical(eng_idx, num_classes=29)\n",
        "print(eng_idx.shape, eng_onehot.shape)\n",
        "\n",
        "fra_onehot = tf.keras.utils.to_categorical(fra_idx, num_classes=44)\n",
        "print(fra_idx.shape, fra_onehot.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 24) (50000, 24, 29)\n",
            "(50000, 73) (50000, 73, 44)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGvKpSjVeXig"
      },
      "source": [
        "## 모델 준비: 모델 생성\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/KerasKorea/KEKOxTutorial/master/media/28_1.png' width=400>\n",
        "\n",
        "- (주의) 디코더 쪽의 입력은 fra[:-1]의 형태 출력은 fra[1:]의 형태로 해야함. \n",
        "- `dec_X`의 입력 모양이 준비한 `fra_onehot` 실제 모양보다 -1의 된 것에 대한 이해 필요. \n",
        "- 학습을 시킬 때에 fra_onehot[:, :-1], fra_onehot[:, 1:] 로 조정하여 입력하게 됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CUw4bfJPz8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ea6119-edba-4245-c8ee-ce2ed8559016"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 영어 - 가장 긴 문장: 24, 사용 알파벳 수: 29, (50000, 24, 29)\n",
        "# 불어 - 가장 긴 문장: 73, 사용 알파벳 수: 44, (50000, 73, 44)\n",
        "\n",
        "# Encoder\n",
        "enc_X = tf.keras.layers.Input(shape=[24, 29])\n",
        "enc_Y, enc_S = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(enc_X)\n",
        "\n",
        "# Decoder\n",
        "dec_X = tf.keras.layers.Input(shape=[72, 44])\n",
        "dec_H = tf.keras.layers.GRU(256, return_sequences=True)(dec_X, initial_state=enc_S)\n",
        "dec_H = tf.keras.layers.Dense(256, activation=\"swish\")(dec_H)\n",
        "dec_Y = tf.keras.layers.Dense(44, activation=\"softmax\")(dec_H)\n",
        "\n",
        "model = tf.keras.models.Model([enc_X, dec_X], dec_Y)\n",
        "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 24, 29)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 72, 44)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       [(None, 24, 256), (N 220416      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 72, 256)      231936      input_2[0][0]                    \n",
            "                                                                 gru[0][1]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 72, 256)      65792       gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 72, 44)       11308       dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 529,452\n",
            "Trainable params: 529,452\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI-9FdGpcWBK"
      },
      "source": [
        "## 모델 준비: 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EMcN3rkf_KO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7753968-db97-49e6-bca2-77a9898f4b60"
      },
      "source": [
        "model.fit([eng_onehot, fra_onehot[:, :-1]], fra_onehot[:, 1:], \n",
        "          batch_size=128, epochs=30)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "391/391 [==============================] - 14s 15ms/step - loss: 1.0789 - accuracy: 0.7293\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.5867 - accuracy: 0.8210\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4737 - accuracy: 0.8545\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.4125 - accuracy: 0.8724\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3744 - accuracy: 0.8841\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3445 - accuracy: 0.8929\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3229 - accuracy: 0.8990\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.3075 - accuracy: 0.9038\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2940 - accuracy: 0.9076\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2824 - accuracy: 0.9113\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2731 - accuracy: 0.9137\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2632 - accuracy: 0.9166\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2567 - accuracy: 0.9185\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2484 - accuracy: 0.9211\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2426 - accuracy: 0.9226\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2376 - accuracy: 0.9241\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2319 - accuracy: 0.9258\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2266 - accuracy: 0.9273\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2225 - accuracy: 0.9286\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2174 - accuracy: 0.9303\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2136 - accuracy: 0.9312\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2096 - accuracy: 0.9326\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2051 - accuracy: 0.9339\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.2016 - accuracy: 0.9349\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1989 - accuracy: 0.9358\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1941 - accuracy: 0.9372\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1911 - accuracy: 0.9379\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1889 - accuracy: 0.9388\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1859 - accuracy: 0.9398\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.1826 - accuracy: 0.9407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f89731002b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dCli5Tgtf_"
      },
      "source": [
        "## 모델 이용: 모델을 이용한 번역"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4h_8-7M1gs7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 영어 - 가장 긴 문장: 24, 사용 알파벳 수: 29\n",
        "# 불어 - 가장 긴 문장: 73, 사용 알파벳 수: 44\n",
        "\n",
        "def translate(eng):\n",
        "    # eng => pad => index => onehot\n",
        "    eng_sent = list(eng.ljust(24))\n",
        "    eng_idx = np.array([eng_token.get(c) for c in eng_sent])\n",
        "    eng_onehot = tf.keras.utils.to_categorical(eng_idx, num_classes=29)\n",
        "\n",
        "    fra = ''\n",
        "    for n in range(72):\n",
        "        # fra => pad => index => onehot\n",
        "        fra_sent = list(f'\\t{fra}'.ljust(72))\n",
        "        fra_indexed = np.array([fra_token.get(c) for c in fra_sent])\n",
        "        fra_onehot = tf.keras.utils.to_categorical(fra_indexed, num_classes=44)\n",
        "\n",
        "        # eng_onehot shape: (24, 29), fra_onehot shape: (72, 44)\n",
        "        # 3차원 형태로 변형하여 모델에 입력해야 함.\n",
        "        fra_next = model.predict([eng_onehot.reshape(1, 24, 29), fra_onehot.reshape(1, 72, 44)])\n",
        "\n",
        "        # onehot -> index -> char\n",
        "        fra = ''.join([fra_char[i] for i in np.argmax(fra_next[0], axis=1)])\n",
        "        # 번역된 char 선택\n",
        "        fra = fra[:n+1]\n",
        "\n",
        "        if fra[-1] == '\\n':\n",
        "            break\n",
        "\n",
        "    return fra"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6sVCKHOxIPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc158ca-f5b4-478b-828e-49166c6de8eb"
      },
      "source": [
        "import random\n",
        "\n",
        "# 랜덤 10개\n",
        "indices = list(range(50000))\n",
        "random.shuffle(indices)\n",
        "\n",
        "for n in indices[:10]:\n",
        "    print(f'영어: {eng_sent[n]}\\n불어: {fra_sent[n][1:-1]}')\n",
        "    print(f'번역: {translate(eng_sent[n])}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어: is the bath ready\n",
            "불어: le bain est il prêt\n",
            "번역: le chat a t il terminé\n",
            "\n",
            "영어: tom scrubbed his feet\n",
            "불어: tom nettoya ses pieds\n",
            "번역: tom a l'air d'un peu de café\n",
            "\n",
            "영어: why does he look grumpy\n",
            "불어: pourquoi a t il l'air grincheux\n",
            "번역: pourquoi le connaît est il là\n",
            "\n",
            "영어: i'm not panicking\n",
            "불어: je ne panique pas\n",
            "번역: je ne suis pas en train de me faire confiance\n",
            "\n",
            "영어: stop\n",
            "불어: arrête toi\n",
            "번역: arrêtez\n",
            "\n",
            "영어: stop bothering me\n",
            "불어: arrête de m'ennuyer\n",
            "번역: arrêtez de m'en alles\n",
            "\n",
            "영어: i'm sick of lying\n",
            "불어: j'en ai marre de mentir\n",
            "번역: j'ai des problèmes\n",
            "\n",
            "영어: he cannot be saved\n",
            "불어: il ne peut être sauvé\n",
            "번역: il ne peut pas le faire ici\n",
            "\n",
            "영어: he lifted her in the air\n",
            "불어: il la souleva en l'air\n",
            "번역: il est allé à la porte ouverte\n",
            "\n",
            "영어: what're you good at\n",
            "불어: à quoi êtes vous bon\n",
            "번역: que cherchez vous\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmkiEY9trAax"
      },
      "source": [
        "# Seq2Seq II - Word Level (without attention)\n",
        "1. 데이터 준비\n",
        "  - 문장을 단어로 분리 \n",
        "  - tokeninzing - 각 단어에 매핑되는 숫자(인덱스) 구성\n",
        "  - indexing\n",
        "  - padding\n",
        "1. 모델 준비 \n",
        "  - 모델 생성\n",
        "  - 모델 학습\n",
        "1. 모델의 이용\n",
        "  - 번역"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkNzd_1QOVBT"
      },
      "source": [
        "## 데이터 준비: 문장을 단어를 기준으로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJs9cOUctlrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccb25ba-a341-4da6-e7ed-041c00a086ef"
      },
      "source": [
        "eng_words = [eng.split() for eng in eng_sent]\n",
        "# start 단어로 '<sos>' 사용, end 단어로 '<eos>' 사용\n",
        "fra_words = [f'<sos> {fra} <eos>'.split() for fra in fra_sent]\n",
        "\n",
        "print(eng_words[:3])\n",
        "print(fra_words[:3])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['it', 'was', 'a', 'mess'], [\"that's\", 'a', 'good', 'guess'], ['my', 'house', 'is', 'your', 'house']]\n",
            "[['<sos>', \"c'était\", 'le', 'bordel', '<eos>'], ['<sos>', 'bien', 'deviné', '<eos>'], ['<sos>', 'ma', 'demeure', 'est', 'la', 'tienne', '<eos>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfVNvyMxOb6M"
      },
      "source": [
        "## 데이터 준비: tokenizing & indexing & padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM5OxM46xYng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ce7ae8-d4f7-4b89-c747-6aecf8d1cbd9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# tokenizing\n",
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer_en.fit_on_texts(eng_words)\n",
        "\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer_fr.fit_on_texts(fra_words)\n",
        "\n",
        "# indexing\n",
        "eng_idx = tokenizer_en.texts_to_sequences(eng_words)\n",
        "fra_idx = tokenizer_fr.texts_to_sequences(fra_words)\n",
        "\n",
        "print(eng_idx[:3])\n",
        "print(fra_idx[:3])\n",
        "\n",
        "# padding\n",
        "eng_pad = tf.keras.preprocessing.sequence.pad_sequences(eng_idx, padding=\"post\")\n",
        "fra_pad = tf.keras.preprocessing.sequence.pad_sequences(fra_idx, padding=\"post\")\n",
        "\n",
        "print(eng_pad.shape)\n",
        "print(fra_pad.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 23, 3, 836], [50, 3, 65, 390], [21, 171, 4, 22, 171]]\n",
            "[[1, 123, 11, 1889, 2], [1, 54, 1740, 2], [1, 61, 1741, 10, 15, 1423, 2]]\n",
            "(50000, 8)\n",
            "(50000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQTN0EmsyWc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098b3471-e78c-44d6-b3fa-c33f952c39a5"
      },
      "source": [
        "# tokenizer에서 0 index가 구성되어있지 않지만, \n",
        "# pad_sequence에서 pad의 의미로 0을 사용하고 있어서, 전체 사이즈를 구할 때, +1을 해준다.\n",
        "\n",
        "enc_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "fra_vocab_size = len(tokenizer_fr.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기: {:d}\\n프랑스어 단어 집합의 크기: {:d}\".format(enc_vocab_size, fra_vocab_size))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 단어 집합의 크기: 6090\n",
            "프랑스어 단어 집합의 크기: 11804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz8zAMMM1FYZ"
      },
      "source": [
        "## 데이터 준비: 원핫인코딩\n",
        "- **메모리가 터짐**\n",
        "- 다른 대안은 없을까? => Embedding\n",
        "- https://wikidocs.net/33520\n",
        "- https://www.tensorflow.org/tutorials/text/word_embeddings#using_the_embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9dc4Q-61RST"
      },
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# eng_onehot = tf.keras.utils.to_categorical(eng_pad, num_classes=6090)\n",
        "# print(eng_pad.shape, eng_onehot.shape)\n",
        "\n",
        "# fra_onehot = tf.keras.utils.to_categorical(fra_pad, num_classes=11804)\n",
        "# print(fra_pad.shape, fra_onehot.shape)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUy02fxkRNkW"
      },
      "source": [
        "## 모델 준비: 모델 생성 - 기본 구조 (without attention)\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/KerasKorea/KEKOxTutorial/master/media/28_1.png' width=400>\n",
        "\n",
        "- (주의) 디코더 쪽의 입력은 fra[:-1]의 형태 출력은 fra[1:]의 형태로 해야함. \n",
        "- `dec_X`의 입력 모양이 준비한 `fra_pad` 실제 모양보다 -1의 된 것에 대한 이해 필요. \n",
        "- 학습을 시킬 때에 fra_pad[:, :-1], fra_pad[:, 1:] 로 조정하여 입력하게 됨.\n",
        "- label이 onehot encoding이 되어있지 않은 경우에, loss를 `sparse_categorical_crossentropy` 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjSqhATl1AdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2318a91e-808e-4292-d1d1-40a0e2f6db7b"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 영어 단어 집합의 크기: 6090, (50000, 8)\n",
        "# 프랑스어 단어 집합의 크기: 11804, (50000, 16)\n",
        "\n",
        "# Encoder\n",
        "enc_X = tf.keras.layers.Input(shape=[8])\n",
        "enc_E = tf.keras.layers.Embedding(6090, 50)(enc_X) # 토큰수, 차원수\n",
        "enc_Y, enc_S = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(enc_E)\n",
        "\n",
        "# Decoder\n",
        "dec_X = tf.keras.layers.Input(shape=[15])\n",
        "dec_E = tf.keras.layers.Embedding(11804, 50)(dec_X) # 토큰수, 차원수\n",
        "dec_H = tf.keras.layers.GRU(256, return_sequences=True)(dec_E, initial_state=enc_S)\n",
        "dec_H = tf.keras.layers.Dense(256, activation=\"swish\")(dec_H)\n",
        "dec_Y = tf.keras.layers.Dense(11804, activation=\"softmax\")(dec_H)\n",
        "\n",
        "model = tf.keras.models.Model([enc_X, dec_X], dec_Y)\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 8, 50)        304500      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 15, 50)       590200      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       [(None, 8, 256), (No 236544      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 15, 256)      236544      embedding_1[0][0]                \n",
            "                                                                 gru[0][1]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 15, 256)      65792       gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 15, 11804)    3033628     dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 4,467,208\n",
            "Trainable params: 4,467,208\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhtRQZnhoYih"
      },
      "source": [
        "## 모델 준비: 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAF8dYf1qJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0673c7eb-4ffc-43a1-8156-1e04445faa6f"
      },
      "source": [
        "model.fit([eng_pad, fra_pad[:, :-1]], fra_pad[:, 1:], \n",
        "          batch_size=128, epochs=5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 17s 38ms/step - loss: 2.7050 - accuracy: 0.6569\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 15s 37ms/step - loss: 1.6590 - accuracy: 0.7406\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 15s 37ms/step - loss: 1.4729 - accuracy: 0.7627\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 15s 38ms/step - loss: 1.3539 - accuracy: 0.7774\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 15s 38ms/step - loss: 1.2641 - accuracy: 0.7882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f89b1542390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCyXPIeYocJT"
      },
      "source": [
        "## 모델 이용: 모델을 이용한 번역"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri18hkTf3aWd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 영어 단어 집합의 크기: 6090, (50000, 8)\n",
        "# 프랑스어 단어 집합의 크기: 11804, (50000, 16)\n",
        "\n",
        "def translate(eng):\n",
        "    # eng => index => pad\n",
        "    eng_idx = tokenizer_en.texts_to_sequences([eng])\n",
        "    eng_pad = tf.keras.preprocessing.sequence.pad_sequences(eng_idx, maxlen=8, padding=\"post\")\n",
        "\n",
        "    fra = []\n",
        "    for n in range(15):\n",
        "        # fra => index => pad\n",
        "        fra_index = tokenizer_fr.texts_to_sequences([['<sos>'] + fra])\n",
        "        fra_pad = tf.keras.preprocessing.sequence.pad_sequences(fra_index, maxlen=15, padding=\"post\")\n",
        "\n",
        "        fra_next = model.predict([eng_pad, fra_pad])\n",
        "\n",
        "        # onehot -> index -> word\n",
        "        fra = [tokenizer_fr.index_word[i] for i in np.argmax(fra_next[0], axis=1) if i != 0]\n",
        "        # 번역된 word 선택\n",
        "        fra = fra[:n+1]\n",
        "        \n",
        "        if fra[-1] == '<eos>':\n",
        "            break\n",
        "\n",
        "    return fra"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOqZ6YdV2XZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6cecc0e-d986-4d0d-a900-3b4999878de9"
      },
      "source": [
        "import random\n",
        "\n",
        "# 랜덤 10개\n",
        "indices = list(range(50000))\n",
        "random.shuffle(indices)\n",
        "\n",
        "for n in indices[:10]:\n",
        "    print(f\"영어: {' '.join(eng_words[n])}\\n불어: {' '.join(fra_words[n][1:-1])}\")\n",
        "    print(f\"번역: {' '.join(translate(eng_words[n])[:-1])}\")\n",
        "    print()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어: i'll be glad to\n",
            "불어: oui avec joie\n",
            "번역: je suis en train de vous\n",
            "\n",
            "영어: tom wanted to help mary\n",
            "불어: tom voulait aider mary\n",
            "번역: tom a fait un peu\n",
            "\n",
            "영어: they're not following\n",
            "불어: elles ne sont pas en train de suivre\n",
            "번역: elles ne sont pas si\n",
            "\n",
            "영어: i owe him my life\n",
            "불어: je lui dois la vie\n",
            "번역: je vous ai un peu de la maison\n",
            "\n",
            "영어: you're still green\n",
            "불어: tu es encore un bleu\n",
            "번역: vous êtes très très heureux\n",
            "\n",
            "영어: the car bumped the tree\n",
            "불어: la voiture a heurté l'arbre\n",
            "번역: le chien est en train de la maison\n",
            "\n",
            "영어: i slept very well\n",
            "불어: je dormis fort bien\n",
            "번역: je me suis senti très heureux\n",
            "\n",
            "영어: i closed my umbrella\n",
            "불어: je fermai mon parapluie\n",
            "번역: je me suis senti très heureux\n",
            "\n",
            "영어: maybe i'll call you\n",
            "불어: peut être t'appellerai je\n",
            "번역: ce n'est pas si\n",
            "\n",
            "영어: can i sit next to you\n",
            "불어: est ce que je peux m'asseoir à côté de toi\n",
            "번역: puis je vous voir\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRcj28YBomTM"
      },
      "source": [
        "# Seq2Seq II - Word Level (with attention)\n",
        "1. 모델 준비 (위 모델을 이용한 개선)\n",
        "  - 모델 생성\n",
        "  - 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-aAbiQ1Aob0"
      },
      "source": [
        "## 모델 준비: 모델 생성 - Attention 구조 (dot prodoct ver)\n",
        "- 커멘트의 설명은 **딥 러닝을 이용한 자연어 처리 입문** > **어텐션 메커니즘 (Attention Mechanism)** 문서를 참고했습니다. \n",
        "  1. 어텐션 스코어(Attention Score)를 구한다.\n",
        "  1. 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\n",
        "  1. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value)을 구한다.\n",
        "  1. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.(Concatenate)\n",
        "  1. 출력층 연산의 입력이 되는 dec_H를 계산합니다.\n",
        "  - https://wikidocs.net/22893"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKGfKfON-bIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088bf7c0-20bb-439e-c43f-910bba77a5dc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# 영어 단어 집합의 크기: 6090, (50000, 8)\n",
        "# 프랑스어 단어 집합의 크기: 11804, (50000, 16)\n",
        "\n",
        "# Encoder\n",
        "enc_X = tf.keras.layers.Input(shape=[8])\n",
        "enc_E = tf.keras.layers.Embedding(6090, 50)(enc_X) # 토큰수, 차원수\n",
        "enc_Y, enc_S = tf.keras.layers.SimpleRNN(256, return_sequences=True, return_state=True)(enc_E)\n",
        "\n",
        "# Decoder\n",
        "dec_X = tf.keras.layers.Input(shape=[15])\n",
        "dec_E = tf.keras.layers.Embedding(11804, 50)(dec_X) # 토큰수, 차원수\n",
        "dec_H = tf.keras.layers.SimpleRNN(256, return_sequences=True)(dec_E, initial_state=enc_S)\n",
        "\n",
        "################################\n",
        "## attention layer\n",
        "################################\n",
        "# key, value, query\n",
        "key = enc_Y  # 인코더의 히든스테이트를 key로 활용한다. (8, 256)\n",
        "value = enc_Y  # 인코더의 히든스테이트를 value로 활용한다. (8, 256)\n",
        "query = dec_H  # 디코더의 히든스테이트를 query로 활용한다. (15, 256)\n",
        "\n",
        "# 1. 어텐션 스코어(Attention Score)를 구한다.\n",
        "score = tf.matmul(query, key, transpose_b=True)\n",
        "# 연산 결과: (15, 256) * (256, 8) => (15, 8)\n",
        "\n",
        "# 2. 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution)를 구한다.\n",
        "att_dist = tf.nn.softmax(score, axis=-1) \n",
        "\n",
        "# 3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value)을 구한다.\n",
        "att_value = tf.matmul(att_dist, value)\n",
        "# 연산 결과: (15, 8) * (8, 256) => (15, 256)\n",
        "\n",
        "# 4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결한다.(Concatenate)\n",
        "dec_H = tf.keras.layers.Concatenate()([att_value, dec_H])\n",
        "\n",
        "# 5. 출력층 연산의 입력이 되는 dec_H를 계산합니다.\n",
        "dec_H = tf.keras.layers.Dense(256, activation='tanh')(dec_H)\n",
        "################################\n",
        "\n",
        "dec_Y = tf.keras.layers.Dense(11804, activation=\"softmax\")(dec_H)\n",
        "\n",
        "model = tf.keras.models.Model([enc_X, dec_X], dec_Y)\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 15)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 8, 50)        304500      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 15, 50)       590200      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn (SimpleRNN)          [(None, 8, 256), (No 78592       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)        (None, 15, 256)      78592       embedding_1[0][0]                \n",
            "                                                                 simple_rnn[0][1]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul (TFOpLambda)   (None, 15, 8)        0           simple_rnn_1[0][0]               \n",
            "                                                                 simple_rnn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.softmax (TFOpLambda)      (None, 15, 8)        0           tf.linalg.matmul[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.linalg.matmul_1 (TFOpLambda) (None, 15, 256)      0           tf.nn.softmax[0][0]              \n",
            "                                                                 simple_rnn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 15, 512)      0           tf.linalg.matmul_1[0][0]         \n",
            "                                                                 simple_rnn_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 15, 256)      131328      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 15, 11804)    3033628     dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 4,216,840\n",
            "Trainable params: 4,216,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKpDhhBo64m"
      },
      "source": [
        "## 모델 준비: 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL5hDzze-qlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0f9fbc-57ed-49de-b65c-264780025b15"
      },
      "source": [
        "model.fit([eng_pad, fra_pad[:, :-1]], fra_pad[:, 1:], \n",
        "          batch_size=128, epochs=30)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "391/391 [==============================] - 18s 43ms/step - loss: 2.5827 - accuracy: 0.6658\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.4841 - accuracy: 0.7716\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.3049 - accuracy: 0.7925\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.1940 - accuracy: 0.8068\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.1040 - accuracy: 0.8181\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 1.0381 - accuracy: 0.8265\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.9774 - accuracy: 0.8348\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.9274 - accuracy: 0.8418\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.8858 - accuracy: 0.8484\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.8496 - accuracy: 0.8540\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.8162 - accuracy: 0.8597\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7934 - accuracy: 0.8645\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7690 - accuracy: 0.8686\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7473 - accuracy: 0.8726\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7342 - accuracy: 0.8758\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7149 - accuracy: 0.8794\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.7079 - accuracy: 0.8818\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6963 - accuracy: 0.8840\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 17s 44ms/step - loss: 0.6798 - accuracy: 0.8871\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6673 - accuracy: 0.8896\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6555 - accuracy: 0.8914\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6441 - accuracy: 0.8937\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6329 - accuracy: 0.8959\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6193 - accuracy: 0.8982\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6126 - accuracy: 0.8994\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.6008 - accuracy: 0.9016\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.5962 - accuracy: 0.9024\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.5865 - accuracy: 0.9040\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.5753 - accuracy: 0.9053\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.5704 - accuracy: 0.9065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f89b33d28d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbArffYo-8l"
      },
      "source": [
        "## 모델 이용: 모델을 이용한 번역"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al0tXZA--wpe"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 영어 단어 집합의 크기: 6090, (50000, 8)\n",
        "# 프랑스어 단어 집합의 크기: 11804, (50000, 16)\n",
        "\n",
        "def translate(eng):\n",
        "    # eng => index => pad\n",
        "    eng_idx = tokenizer_en.texts_to_sequences([eng])\n",
        "    eng_pad = tf.keras.preprocessing.sequence.pad_sequences(eng_idx, maxlen=8, padding=\"post\")\n",
        "\n",
        "    fra = []\n",
        "    for n in range(15):\n",
        "        # fra => index => pad\n",
        "        fra_index = tokenizer_fr.texts_to_sequences([['<sos>'] + fra])\n",
        "        fra_pad = tf.keras.preprocessing.sequence.pad_sequences(fra_index, maxlen=15, padding=\"post\")\n",
        "\n",
        "        fra_next = model.predict([eng_pad, fra_pad])\n",
        "\n",
        "        # onehot -> index -> word\n",
        "        fra = [tokenizer_fr.index_word[i] for i in np.argmax(fra_next[0], axis=1) if i != 0]\n",
        "        # 번역된 word 선택\n",
        "        fra = fra[:n+1]\n",
        "\n",
        "        if fra[-1] == '<eos>':\n",
        "            break\n",
        "\n",
        "    return fra"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_RftjLAAup5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ca1463-4cac-44e5-957f-1c28a1ad81e9"
      },
      "source": [
        "import random\n",
        "\n",
        "# 랜덤 10개\n",
        "indices = list(range(50000))\n",
        "random.shuffle(indices)\n",
        "\n",
        "for n in indices[:10]:\n",
        "    print(f\"영어: {' '.join(eng_words[n])}\\n불어: {' '.join(fra_words[n][1:-1])}\")\n",
        "    print(f\"번역: {' '.join(translate(eng_words[n])[:-1])}\")\n",
        "    print()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어: it feels like a dream\n",
            "불어: on se sent comme dans un rêve\n",
            "번역: on se sent une très pris\n",
            "\n",
            "영어: treat her well\n",
            "불어: traite la bien\n",
            "번역: cela se passe\n",
            "\n",
            "영어: tom is also an artist\n",
            "불어: tom est aussi artiste\n",
            "번역: tom est aussi jeune\n",
            "\n",
            "영어: it was almost funny\n",
            "불어: c'était presque amusant\n",
            "번역: c'était presque amusant\n",
            "\n",
            "영어: i'll go look\n",
            "불어: j'irai voir\n",
            "번역: j'irai en un\n",
            "\n",
            "영어: you leave me no choice\n",
            "불어: vous ne me laissez aucun choix\n",
            "번역: tu ne me le aucun choix\n",
            "\n",
            "영어: we all deserve to go\n",
            "불어: nous méritons tous d'y aller\n",
            "번역: nous voulons tous partir\n",
            "\n",
            "영어: everyone looks worried\n",
            "불어: tout le monde a l'air soucieux\n",
            "번역: tout le monde a l'air malade\n",
            "\n",
            "영어: is that a promise\n",
            "불어: s'agit il d'une promesse\n",
            "번역: s'agit il d'une quelque chose\n",
            "\n",
            "영어: my life is in your hands\n",
            "불어: ma vie repose entre vos mains\n",
            "번역: ma vie est sur lui peur\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVMM7qnAo_z9"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}