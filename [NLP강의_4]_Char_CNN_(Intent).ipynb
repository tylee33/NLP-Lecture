{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[NLP강의-4] Char-CNN (Intent).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylee33/NLP-Lecture/blob/master/%5BNLP%EA%B0%95%EC%9D%98_4%5D_Char_CNN_(Intent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-4OXELSy19k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "155f5659-fb92-4e6d-93ea-6ab39bdfda84"
      },
      "source": [
        "import requests\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.image import imread, imsave \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd\n",
        "from konlpy.tag import Twitter\n",
        "from gensim.models import word2vec\n",
        "print(\"load done\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F36xTboZzEnA",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t79pn2ykzB0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "366214b2-cdef-47b9-997a-e1db3d647a02"
      },
      "source": [
        "vector_size = 50\n",
        "encode_length = 4\n",
        "label_size = 3\n",
        "embed_type = \"onehot\" #onehot or w2v\n",
        "# Choose single test\n",
        "# filter_type = \"single\"\n",
        "# filter_number = 32\n",
        "# filter_size = 2\n",
        "\n",
        "# Choose multi test\n",
        "filter_type = \"multi\"\n",
        "filter_sizes = [2,3,4,2,3,4,2,3,4]\n",
        "num_filters = len(filter_sizes)\n",
        "\n",
        "train_data_list =  {\n",
        "                'encode' : ['판교에 오늘 피자 주문해줘','오늘 날짜에 호텔 예약 해줄레','모래 날짜에 판교 여행 정보 알려줘'],\n",
        "                'decode' : ['0','1','2']\n",
        "             }\n",
        "train_data_list.get('encode')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['판교에 오늘 피자 주문해줘', '오늘 날짜에 호텔 예약 해줄레', '모래 날짜에 판교 여행 정보 알려줘']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1pslggLzMrY",
        "colab_type": "text"
      },
      "source": [
        "# Vector model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJY2pDBuzQRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "b916436d-dbfe-464e-f71d-0547410f86c9"
      },
      "source": [
        "def train_vector_model(str_buf):\n",
        "  \n",
        "    twitter = Twitter()\n",
        "    str_buf = train_data_list['encode']\n",
        "    pos1 = twitter.pos(''.join(str_buf))\n",
        "    print(pos1)\n",
        "    pos2 = ' '.join(list(map(lambda x : '\\n' if x[1] in ['SF'] else x[0], pos1))).split('\\n')\n",
        "    print(pos2)\n",
        "    morphs = list(map(lambda x : twitter.morphs(x) , pos2))\n",
        "    print(morphs)\n",
        "    print(str_buf)\n",
        "    model = word2vec.Word2Vec(size=vector_size, window=2, min_count=1)\n",
        "#    model.build_vocab(morphs)\n",
        "#    model.train(morphs)\n",
        "    return model\n",
        "\n",
        "model = train_vector_model(train_data_list)\n",
        "print(model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
            "W0815 06:01:19.758012 140642289616768 base_any2vec.py:723] consider setting layer size to a multiple of 4 for greater performance\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('판교', 'Noun'), ('에', 'Josa'), ('오늘', 'Noun'), ('피자', 'Noun'), ('주문', 'Noun'), ('해줘', 'Verb'), ('오늘', 'Noun'), ('날짜', 'Noun'), ('에', 'Josa'), ('호텔', 'Noun'), ('예약', 'Noun'), ('해줄', 'Verb'), ('레', 'Noun'), ('모래', 'Noun'), ('날짜', 'Noun'), ('에', 'Josa'), ('판교', 'Noun'), ('여행', 'Noun'), ('정보', 'Noun'), ('알려줘', 'Verb')]\n",
            "['판교 에 오늘 피자 주문 해줘 오늘 날짜 에 호텔 예약 해줄 레 모래 날짜 에 판교 여행 정보 알려줘']\n",
            "[['판교', '에', '오늘', '피자', '주문', '해줘', '오늘', '날짜', '에', '호텔', '예약', '해줄', '레', '모래', '날짜', '에', '판교', '여행', '정보', '알려줘']]\n",
            "['판교에 오늘 피자 주문해줘', '오늘 날짜에 호텔 예약 해줄레', '모래 날짜에 판교 여행 정보 알려줘']\n",
            "Word2Vec(vocab=0, size=50, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT19U7SJ1wAx",
        "colab_type": "text"
      },
      "source": [
        "# Load Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YRYEz0b1yx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_csv(data_path):\n",
        "    df_csv_read = pd.DataFrame(data_path)\n",
        "    return df_csv_read"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQHiL6EW12Wv",
        "colab_type": "text"
      },
      "source": [
        "# Embed word to vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZy9QMQ14Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed(data) : \n",
        "    twitter = Twitter()\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    for encode_raw in data['encode'] : \n",
        "        encode_raw = twitter.morphs(encode_raw)\n",
        "        encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
        "        if(embed_type == 'onehot') :\n",
        "            bucket = np.zeros(vector_size, dtype=float).copy()\n",
        "            input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
        "        else : \n",
        "            input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
        "        inputs.append(input.flatten())\n",
        "        \n",
        "    for decode_raw in data['decode']: \n",
        "        label = np.zeros(label_size, dtype=float)\n",
        "        np.put(label, decode_raw, 1)\n",
        "        labels.append(label)\n",
        "    return inputs, labels\n",
        "\n",
        "def onehot_vectorize(bucket, x):\n",
        "    np.put(bucket, model.wv.index2word.index(x),1)\n",
        "    return bucket\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPZ8ewtq18Mf",
        "colab_type": "text"
      },
      "source": [
        "# Embed word to vector on predict step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHqLUoM1-qH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_embed(data) : \n",
        "    twitter = Twitter()\n",
        "    encode_raw = twitter.morphs(data)\n",
        "    encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
        "    if(embed_type == 'onehot') :\n",
        "        bucket = np.zeros(vector_size, dtype=float).copy()\n",
        "        input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
        "    else : \n",
        "        input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
        "    return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnubOswP2Cw3",
        "colab_type": "text"
      },
      "source": [
        "# get train and test data for feed on tensorflow session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkm0-YLi2Dkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_data():\n",
        "    train_data, train_label = embed(load_csv(train_data_list))\n",
        "    test_data, test_label = embed(load_csv(train_data_list))\n",
        "    return train_label, test_label, train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nByg2TUF2GRP",
        "colab_type": "text"
      },
      "source": [
        "# create graph with single filter size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SEfXh5a2IwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ef3b23b-7f55-4594-aff2-22a19f4c6126"
      },
      "source": [
        "def create_s_graph(train=True):\n",
        "    # placeholder is used for feeding data.\n",
        "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
        "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
        "\n",
        "    # reshape input data\n",
        "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
        "    \n",
        "    # Build a convolutional layer and maxpooling with random initialization\n",
        "    W_conv1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, 1, filter_number], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
        "    b_conv1 = tf.Variable(tf.zeros([filter_number]), name=\"b_conv1\")\n",
        "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
        "    h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
        "    \n",
        "    # Build a fully connected layer\n",
        "    h_pool2_flat = tf.reshape(h_pool1, [-1, int((encode_length/2)*(vector_size/2))*filter_number], name=\"h_pool2_flat\")\n",
        "    W_fc1 = tf.Variable(tf.truncated_normal([int((encode_length/2)*(vector_size/2))*filter_number, 256], stddev=0.1), name = 'W_fc1')\n",
        "    b_fc1 = tf.Variable(tf.zeros([256]), name = 'b_fc1')\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
        "    \n",
        "    keep_prob = 1.0\n",
        "    if(train) : \n",
        "        # Dropout Layer\n",
        "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
        "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
        "    \n",
        "    # Build a fully connected layer with softmax \n",
        "    W_fc2 = tf.Variable(tf.truncated_normal([256, label_size], stddev=0.1), name = 'W_fc2')\n",
        "    b_fc2 = tf.Variable(tf.zeros([label_size]), name = 'b_fc2')\n",
        "    #y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
        "    y=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "    \n",
        "    # define the Loss function\n",
        "    #cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target))\n",
        "    \n",
        "    # define optimization algorithm\n",
        "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "\n",
        "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
        "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
        "    # tf.cast() : changes true -> 1 / false -> 0\n",
        "    # tf.reduce_mean() : calculate the mean\n",
        "    \n",
        "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
        "    \n",
        "print(\"define cnn graph func\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "define cnn graph func\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRtCsxI-2N7w",
        "colab_type": "text"
      },
      "source": [
        "# create graph with multi filter size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InYhsnuX2QCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "838996a9-02ea-45ab-b922-c762ba649c28"
      },
      "source": [
        "def create_m_graph(train=True):\n",
        "    # placeholder is used for feeding data.\n",
        "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
        "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
        "\n",
        "    # reshape input data\n",
        "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
        "    # Keeping track of l2 regularization loss (optional)\n",
        "    l2_loss = tf.constant(0.0)\n",
        "    \n",
        "    pooled_outputs = []\n",
        "    for i, filter_size in enumerate(filter_sizes):\n",
        "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "            # Convolution Layer\n",
        "            filter_shape = [filter_size, vector_size, 1, num_filters]\n",
        "            W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "            b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "            \n",
        "            conv = tf.nn.conv2d(\n",
        "                x_image,\n",
        "                W_conv1,\n",
        "                strides=[1, 1, 1, 1],\n",
        "                padding=\"VALID\",\n",
        "                name=\"conv\")\n",
        "            \n",
        "            # Apply nonlinearity\n",
        "            h = tf.nn.relu(tf.nn.bias_add(conv, b_conv1), name=\"relu\")\n",
        "            # Maxpooling over the outputs\n",
        "            pooled = tf.nn.max_pool(\n",
        "                h,\n",
        "                ksize=[1, encode_length - filter_size + 1, 1, 1],\n",
        "                strides=[1, 1, 1, 1],\n",
        "                padding='VALID',\n",
        "                name=\"pool\")\n",
        "            pooled_outputs.append(pooled)\n",
        "\n",
        "    # Combine all the pooled features\n",
        "    num_filters_total = num_filters * len(filter_sizes)\n",
        "    h_pool = tf.concat(pooled_outputs, 3)\n",
        "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
        " \n",
        "    # Add dropout\n",
        "    keep_prob = 1.0\n",
        "    if(train) : \n",
        "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
        "        h_pool_flat = tf.nn.dropout(h_pool_flat, keep_prob)\n",
        "\n",
        "    # Final (unnormalized) scores and predictions\n",
        "    W_fc1 = tf.get_variable(\n",
        "        \"W_fc1\",\n",
        "        shape=[num_filters_total, label_size],\n",
        "        initializer=tf.contrib.layers.xavier_initializer())\n",
        "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[label_size]), name=\"b\")\n",
        "    l2_loss += tf.nn.l2_loss(W_fc1)\n",
        "    l2_loss += tf.nn.l2_loss(b_fc1)\n",
        "    y = tf.nn.xw_plus_b(h_pool_flat, W_fc1, b_fc1, name=\"scores\")\n",
        "    predictions = tf.argmax(y, 1, name=\"predictions\")\n",
        "\n",
        "    # CalculateMean cross-entropy loss\n",
        "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target)\n",
        "    cross_entropy = tf.reduce_mean(losses)\n",
        "\n",
        "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "    \n",
        "    # Accuracy\n",
        "    correct_predictions = tf.equal(predictions, tf.argmax(y_target, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "    \n",
        "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
        "    \n",
        "print(\"define cnn graph func\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "define cnn graph func\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShnShAI2TVX",
        "colab_type": "text"
      },
      "source": [
        "# visualize weight matrix function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cMTHmz2WrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_layer(weight_list) :\n",
        "    if(filter_type == 'multi') : \n",
        "        show = np.array(weight_list).reshape(num_filters, filter_sizes[np.argmax(filter_sizes)], vector_size)\n",
        "        for i, matrix in enumerate(show) :\n",
        "            fig = plt.figure()\n",
        "            plt.imshow(matrix)\n",
        "        plt.show()\n",
        "    else : \n",
        "        show = np.array(weight_list).reshape(32, 2, 2)\n",
        "        for i, matrix in enumerate(show) :\n",
        "            fig = plt.figure()\n",
        "            plt.imshow(matrix)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ABt4r0M2ZkH",
        "colab_type": "text"
      },
      "source": [
        "# run train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXTI1ie2b-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "9796d08a-71d1-44b1-cb77-8bf702effdf2"
      },
      "source": [
        "def run() : \n",
        "    try : \n",
        "        # get Data \n",
        "        labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
        "        # reset Graph\n",
        "        tf.reset_default_graph()   \n",
        " \n",
        "        # Create Session\n",
        "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
        "        # create graph\n",
        "        if(filter_type == 'single') :\n",
        "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_s_graph(train=True)\n",
        "        else :\n",
        "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_m_graph(train=True)\n",
        "            \n",
        "        # set saver\n",
        "        saver = tf.train.Saver(tf.all_variables())\n",
        "        # initialize the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "        # training the MLP\n",
        "        for i in range(200): \n",
        "            sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
        "            if i%10 == 0:\n",
        "                train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
        "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
        "                \n",
        "        # for given x, y_target data set\n",
        "        print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
        "        \n",
        "        # show weight matrix as image \n",
        "        weight_vectors = sess.run(W_conv1, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 1.0})\n",
        "        #show_layer(weight_vectors)\n",
        "        \n",
        "        # Save Model\n",
        "        path = './model/'\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            print(\"path created\")\n",
        "        saver.save(sess, path)\n",
        "        print(\"model saved\")\n",
        "    except Exception as e : \n",
        "        raise Exception (\"error on training: {0}\".format(e))\n",
        "    finally :\n",
        "        sess.close()\n",
        "\n",
        "# run stuff\n",
        "run()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
            "W0815 06:01:46.114734 140642289616768 deprecation.py:506] From <ipython-input-39-7b9853ecf6c3>:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0815 06:01:47.047105 140642289616768 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0815 06:01:47.067095 140642289616768 deprecation.py:323] From <ipython-input-39-7b9853ecf6c3>:60: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0815 06:01:47.480050 140642289616768 deprecation.py:323] From <ipython-input-41-e2910887040c>:17: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Please use tf.global_variables instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy: 0.333\n",
            "step 10, training accuracy: 0.333\n",
            "step 20, training accuracy: 0.333\n",
            "step 30, training accuracy: 0.333\n",
            "step 40, training accuracy: 0.333\n",
            "step 50, training accuracy: 0.333\n",
            "step 60, training accuracy: 0.333\n",
            "step 70, training accuracy: 0.333\n",
            "step 80, training accuracy: 0.333\n",
            "step 90, training accuracy: 0.333\n",
            "step 100, training accuracy: 0.333\n",
            "step 110, training accuracy: 0.333\n",
            "step 120, training accuracy: 0.333\n",
            "step 130, training accuracy: 0.333\n",
            "step 140, training accuracy: 0.333\n",
            "step 150, training accuracy: 0.333\n",
            "step 160, training accuracy: 0.333\n",
            "step 170, training accuracy: 0.333\n",
            "step 180, training accuracy: 0.333\n",
            "step 190, training accuracy: 0.333\n",
            "test accuracy: 0.333333\n",
            "path created\n",
            "model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCfUTt1a3mTZ",
        "colab_type": "text"
      },
      "source": [
        "# predict test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPFbJ89S3juQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "d211f80f-8a80-4c14-a217-fed68c51d9c5"
      },
      "source": [
        "def predict(test_data) : \n",
        "    try : \n",
        "        # reset Graph\n",
        "        tf.reset_default_graph()   \n",
        "        # Create Session\n",
        "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
        "        # create graph\n",
        "        if(filter_type == 'single') :\n",
        "            _, x, _, _, _, y, _, _ = create_s_graph(train=False)\n",
        "        else : \n",
        "            _, x, _, _, _, y, _, _ = create_m_graph(train=False)\n",
        "        \n",
        "        # initialize the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        # set saver\n",
        "        saver = tf.train.Saver()\n",
        "        \n",
        "        # Restore Model\n",
        "        path = './model/'\n",
        "        if os.path.exists(path):\n",
        "            saver.restore(sess, path)\n",
        "            print(\"model restored\")\n",
        "\n",
        "        # training the MLP\n",
        "        #print(\"input data : {0}\".format(test_data))\n",
        "        y = sess.run([y], feed_dict={x: np.array([test_data])})\n",
        "        print(\"result : {0}\".format(y))\n",
        "        print(\"result : {0}\".format(np.argmax(y)))\n",
        "        \n",
        "    except Exception as e : \n",
        "        raise Exception (\"error on training: {0}\".format(e))\n",
        "    finally :\n",
        "        sess.close()\n",
        "\n",
        "print(\"words in dict : {0}\".format(model.wv.index2word))\n",
        "\n",
        "predict(np.array(inference_embed(\"판교에 오늘 피자 주문해줘레\")).flatten())\n",
        "predict(np.array(inference_embed(\"오늘 날짜에 호텔 예약 해줄수있어\")).flatten())\n",
        "predict(np.array(inference_embed(\"모래 날짜에 판교 여행 정보 알려줘\")).flatten())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words in dict : []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
            "W0815 06:02:24.351884 140642289616768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model restored\n",
            "result : [array([[0.06921044, 0.06676348, 0.09685715]], dtype=float32)]\n",
            "result : 2\n",
            "model restored\n",
            "result : [array([[0.06921044, 0.06676348, 0.09685715]], dtype=float32)]\n",
            "result : 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model restored\n",
            "result : [array([[0.06921044, 0.06676348, 0.09685715]], dtype=float32)]\n",
            "result : 2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}